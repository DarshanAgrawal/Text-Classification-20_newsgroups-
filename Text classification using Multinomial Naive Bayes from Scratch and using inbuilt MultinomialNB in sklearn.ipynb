{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Provide path of the folder where all news groups are present\n",
    "path = \"20_newsgroups\"\n",
    "data={} # data is a dictionary of the form { folder1 : [doc1,doc2,....,doc1000] , folder2 : [doc1,doc2,doc3,....] }\n",
    "groups = os.listdir(path)\n",
    "## Bulding the dictionary of documents\n",
    "for group in groups:\n",
    "    data[group]=[]\n",
    "    files = os.listdir(os.path.join(path,group))\n",
    "    for file in files:\n",
    "        with open(os.path.join(path,group,file)) as opened_file:\n",
    "            data[group].append(opened_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Y_Label column\n",
    "Y = []\n",
    "for group in groups:\n",
    "    for i in range(1000):\n",
    "        Y.append(group)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(txt):\n",
    "    new_txt = \"\".join([c for c in txt  if c not in string.punctuation])\n",
    "    return new_txt\n",
    "\n",
    "dicty = {}\n",
    "vocabulary = set()\n",
    "set_of_stop_words = set(stopwords.words(\"english\"))\n",
    "irrelevant_words = [\"the\", \"from\", \"i\",\"lines\",\"subject\", \"organization\", \"in\" ,\"would\", \"this\",\"it\",\"dont\",\"also\", ]\n",
    "\n",
    "## Creating word:frequency dictionary\n",
    "for group in [*data.keys()]:\n",
    "    for doc in data[group]:    \n",
    "        doc = remove_punctuation(doc)  ## Removing the punctuation from the documents\n",
    "        tokenized_words = word_tokenize(doc)  ## Tokenizing the document   \n",
    "        tokenized_words_excluding_stop_words_list = []\n",
    "        for w in tokenized_words:\n",
    "            if (w.lower() not in set_of_stop_words) and (w.lower() not in irrelevant_words) :\n",
    "                dicty[w.lower()] = dicty.get(w.lower(),0) + 1\n",
    "    dicty = dict(sorted(dicty.items(),key=lambda x:x[1],reverse = True))\n",
    "dct = dict(list(dicty.items())[:5000]) ## Selecting the top 5000 words for building the vocabulary\n",
    "features = [*dct.keys()]\n",
    "#dicty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the vocabulary of top 5000 words\n",
    "X = np.zeros([20000,len(features)], dtype = int) \n",
    "## Filling the X_train matrix\n",
    "i = 0\n",
    "for group in [*data.keys()]:\n",
    "    for j in range(len(data[group])): \n",
    "        doc = data[group][j]\n",
    "        doc = remove_punctuation(doc)\n",
    "        tokenized_words = word_tokenize(doc)\n",
    "        for word in tokenized_words:\n",
    "            if word in features:\n",
    "                index = features.index(word)\n",
    "                X[i][index] += 1\n",
    "        i += 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the X dataset to an external csv file for further usage\n",
    "np.savetxt(\"text_classification_data_X.csv\", X,fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the X data from the saved file\n",
    "X = np.genfromtxt('text_classification_data_X.csv', delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the data into training and testing data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.3,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using inbuilt sklearn MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score :  0.8375\n",
      "Confusion Matrix :  [[199   1   0   0   0   0   1   1   4   0   1   1   0   1   1   7   2   2\n",
      "    8  45]\n",
      " [  0 242  25   4   8   5   5   1   2   0   0   1   1   2   3   0   0   0\n",
      "    1   1]\n",
      " [  0  12 259  14   2  11   3   0   0   1   0   1   2   0   1   0   0   0\n",
      "    2   0]\n",
      " [  0   3   7 246  15   0   7   2   0   0   0   0   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   7  15 261   0   9   0   0   0   0   0   4   0   0   0   0   0\n",
      "    0   1]\n",
      " [  0  15   9   3   3 258   2   0   2   0   1   0   1   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   1   1   7   6   0 262  11   4   2   1   3   5   1   5   0   0   1\n",
      "    0   1]\n",
      " [  0   3   0   0   0   1   9 292   5   0   2   0   2   1   2   1   4   0\n",
      "    0   2]\n",
      " [  1   1   0   0   2   0   8   2 285   0   0   0   1   1   2   0   1   0\n",
      "    2   0]\n",
      " [  2   3   0   0   0   0   0   1   1 274   5   0   0   1   0   1   0   0\n",
      "    2   0]\n",
      " [  0   1   1   0   0   0   1   1   3  14 257   0   1   0   0   0   1   0\n",
      "    2   1]\n",
      " [  0   4   1   3   2   2   0   1   0   1   1 256   1   2   1   0   2   0\n",
      "    5   1]\n",
      " [  0  14   5   8   8   1   3   9   2   0   0   1 262   0   0   0   0   0\n",
      "    0   0]\n",
      " [  2   6   1   3   0   0   1   3   3   0   0   0   4 269   4   0   0   1\n",
      "    3   0]\n",
      " [  3   7   1   0   1   0   2   1   2   0   0   1   2   4 256   0   1   1\n",
      "    8   6]\n",
      " [  3   0   0   1   1   0   1   0   0   0   0   0   0   1   1 301   0   0\n",
      "    2   0]\n",
      " [  0   3   0   1   1   0   2   1   2   0   0   2   0   1   1   1 257   3\n",
      "   14   3]\n",
      " [  7   1   1   1   0   0   8   1   2   1   0   0   0   1   1   2   5 287\n",
      "   20   5]\n",
      " [  8   1   2   0   0   0   2   0   1   6   0   2   0   4   7   5  33  19\n",
      "  169  32]\n",
      " [ 79   2   0   1   1   0   3   0   3   1   2   0   0   2   2  17  29   1\n",
      "   22 133]]\n",
      "Classification Report :                            precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.65      0.73      0.69       274\n",
      "           comp.graphics       0.76      0.80      0.78       301\n",
      " comp.os.ms-windows.misc       0.81      0.84      0.82       308\n",
      "comp.sys.ibm.pc.hardware       0.80      0.87      0.83       284\n",
      "   comp.sys.mac.hardware       0.84      0.88      0.86       297\n",
      "          comp.windows.x       0.93      0.87      0.90       295\n",
      "            misc.forsale       0.80      0.84      0.82       311\n",
      "               rec.autos       0.89      0.90      0.90       324\n",
      "         rec.motorcycles       0.89      0.93      0.91       306\n",
      "      rec.sport.baseball       0.91      0.94      0.93       290\n",
      "        rec.sport.hockey       0.95      0.91      0.93       283\n",
      "               sci.crypt       0.96      0.90      0.93       283\n",
      "         sci.electronics       0.90      0.84      0.87       313\n",
      "                 sci.med       0.92      0.90      0.91       300\n",
      "               sci.space       0.89      0.86      0.88       296\n",
      "  soc.religion.christian       0.90      0.97      0.93       311\n",
      "      talk.politics.guns       0.77      0.88      0.82       292\n",
      "   talk.politics.mideast       0.91      0.84      0.87       343\n",
      "      talk.politics.misc       0.65      0.58      0.61       291\n",
      "      talk.religion.misc       0.58      0.45      0.50       298\n",
      "\n",
      "                accuracy                           0.84      6000\n",
      "               macro avg       0.84      0.84      0.83      6000\n",
      "            weighted avg       0.84      0.84      0.84      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score : \", clf.score(X_test,Y_test))\n",
    "print(\"Confusion Matrix : \",confusion_matrix(Y_test, Y_pred))\n",
    "print(\"Classification Report : \",classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB code from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting the model\n",
    "def fit(X_train, Y_train):\n",
    "    result = {}\n",
    "    result[\"total_rows_count\"] = len(Y_train)\n",
    "    num_of_features = X_train.shape[1]\n",
    "    class_labels = set(Y_train)\n",
    "    for current_class_label in class_labels:\n",
    "        result[current_class_label] = {}\n",
    "        current_class_rows = (Y_train == current_class_label)\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        result[current_class_label][\"current_class_rows_count\"] = len(Y_train_current)\n",
    "        result[current_class_label][\"total_words_in_current_class\"] = np.sum(X_train_current)        \n",
    "        for i in range(num_of_features):\n",
    "            result[current_class_label][i] = (X_train_current[:,i]).sum()\n",
    "    return result\n",
    "\n",
    "## Predicting the output class on the testing data\n",
    "def probability(dictionary, x, current_class):\n",
    "    #prior_prob_of_current_class = dictionary[current_class][\"current_class_rows_count\"]/dictionary[\"total_rows_count\"]\n",
    "    prior_prob_of_current_class = np.log(dictionary[current_class][\"total_words_in_current_class\"]) - np.log(dictionary[\"total_rows_count\"])\n",
    "    final_prob = prior_prob_of_current_class\n",
    "    num_of_features = len(dictionary[current_class].keys()) - 2  ## -2 because it also contains \"current_class_rows_count\" & \"total_words_in_current_classalso\n",
    "    for i in range(num_of_features):        \n",
    "        ## Probability with Laplace correction and then Taking log of that\n",
    "        a = dictionary[current_class][i] + 1\n",
    "        b = dictionary[current_class][\"total_words_in_current_class\"] + num_of_features\n",
    "        final_prob += (x[i]*(np.log(a) - np.log(b)))  ## a = count of \"word\" in the current class & b = total count of all words in the current class\n",
    "    return final_prob\n",
    "\n",
    "\n",
    "def predict_prob_single_point(dictionary , x):\n",
    "    final_ans = -9999999\n",
    "    best_class = \"\"\n",
    "    first_run = True\n",
    "    for current_class in [*dictionary.keys()]:\n",
    "        if(current_class == \"total_rows_count\"):\n",
    "            continue\n",
    "        predicted_ans = probability(dictionary, x, current_class)\n",
    "        if(first_run or predicted_ans > final_ans):\n",
    "            final_ans = predicted_ans\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class\n",
    "\n",
    "def predict(dictionary, X_test):\n",
    "    Y_pred = []\n",
    "    for x in X_test:\n",
    "        predicted_class = predict_prob_single_point(dictionary, x)\n",
    "        Y_pred.append(predicted_class)\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :  [[200   1   0   0   0   0   0   1   4   0   1   1   0   1   1   7   2   2\n",
      "    8  45]\n",
      " [  0 241  24   4   8   7   5   0   2   0   0   1   1   2   4   0   0   0\n",
      "    1   1]\n",
      " [  0  13 255  14   2  13   3   0   0   1   0   2   2   0   1   0   0   0\n",
      "    2   0]\n",
      " [  0   3   7 246  15   0   7   2   0   0   0   0   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   7  16 259   0   9   0   0   0   0   0   5   0   0   0   0   0\n",
      "    0   1]\n",
      " [  0  15   8   3   3 259   1   0   2   0   1   0   2   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   1   1   7   7   0 261  11   4   2   1   3   5   1   5   0   0   1\n",
      "    0   1]\n",
      " [  0   3   0   0   0   1   8 292   5   0   3   0   2   1   2   1   4   0\n",
      "    0   2]\n",
      " [  1   1   0   0   2   0   7   2 285   1   0   0   1   1   1   0   1   0\n",
      "    3   0]\n",
      " [  2   3   0   0   0   0   0   1   1 274   5   0   0   1   0   1   0   0\n",
      "    2   0]\n",
      " [  0   1   1   0   0   0   1   1   2  13 259   0   1   0   0   0   1   0\n",
      "    2   1]\n",
      " [  0   4   1   3   2   2   0   1   0   1   1 257   1   2   0   0   2   0\n",
      "    6   0]\n",
      " [  0  14   5   8   8   1   3   9   2   0   0   1 262   0   0   0   0   0\n",
      "    0   0]\n",
      " [  2   6   1   3   0   0   1   3   3   0   0   0   2 271   4   0   0   1\n",
      "    3   0]\n",
      " [  3   7   1   0   1   0   2   1   2   0   0   2   1   4 256   0   1   1\n",
      "    8   6]\n",
      " [  3   0   0   0   1   0   1   0   0   0   0   0   0   1   1 302   0   0\n",
      "    2   0]\n",
      " [  0   4   0   0   1   0   2   1   1   0   0   2   0   1   1   1 256   3\n",
      "   16   3]\n",
      " [  7   1   2   1   0   0   4   1   2   1   0   0   0   1   1   2   4 290\n",
      "   20   6]\n",
      " [  8   1   2   0   0   0   2   0   1   5   0   2   0   4   6   5  32  20\n",
      "  173  30]\n",
      " [ 78   2   0   1   1   0   3   0   3   1   2   0   0   2   2  19  29   1\n",
      "   24 130]]\n",
      "Classification Report :                            precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.66      0.73      0.69       274\n",
      "           comp.graphics       0.75      0.80      0.77       301\n",
      " comp.os.ms-windows.misc       0.81      0.83      0.82       308\n",
      "comp.sys.ibm.pc.hardware       0.80      0.87      0.83       284\n",
      "   comp.sys.mac.hardware       0.84      0.87      0.85       297\n",
      "          comp.windows.x       0.92      0.88      0.90       295\n",
      "            misc.forsale       0.82      0.84      0.83       311\n",
      "               rec.autos       0.90      0.90      0.90       324\n",
      "         rec.motorcycles       0.89      0.93      0.91       306\n",
      "      rec.sport.baseball       0.92      0.94      0.93       290\n",
      "        rec.sport.hockey       0.95      0.92      0.93       283\n",
      "               sci.crypt       0.95      0.91      0.93       283\n",
      "         sci.electronics       0.91      0.84      0.87       313\n",
      "                 sci.med       0.92      0.90      0.91       300\n",
      "               sci.space       0.90      0.86      0.88       296\n",
      "  soc.religion.christian       0.89      0.97      0.93       311\n",
      "      talk.politics.guns       0.77      0.88      0.82       292\n",
      "   talk.politics.mideast       0.91      0.85      0.88       343\n",
      "      talk.politics.misc       0.64      0.59      0.62       291\n",
      "      talk.religion.misc       0.58      0.44      0.50       298\n",
      "\n",
      "                accuracy                           0.84      6000\n",
      "               macro avg       0.84      0.84      0.84      6000\n",
      "            weighted avg       0.84      0.84      0.84      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dictionary = fit(X_train, np.array(Y_train))\n",
    "Y_pred = predict(dictionary, X_test)\n",
    "#print(Y_pred)\n",
    "print(\"Confusion Matrix : \",confusion_matrix(Y_test, Y_pred))\n",
    "print(\"Classification Report : \",classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of inbuilt sklearn MultinomialNB & my code from scratch\n",
    "\n",
    "### Inbuilt Classifier\n",
    "Accuracy = 84%\n",
    "\n",
    "Confusion Matrix :  [[199   1   0   0   0   0   1   1   4   0   1   1   0   1   1   7   2   2\n",
    "    8  45]\n",
    " [  0 242  25   4   8   5   5   1   2   0   0   1   1   2   3   0   0   0\n",
    "    1   1]\n",
    " [  0  12 259  14   2  11   3   0   0   1   0   1   2   0   1   0   0   0\n",
    "    2   0]\n",
    " [  0   3   7 246  15   0   7   2   0   0   0   0   4   0   0   0   0   0\n",
    "    0   0]\n",
    " [  0   0   7  15 261   0   9   0   0   0   0   0   4   0   0   0   0   0\n",
    "    0   1]\n",
    " [  0  15   9   3   3 258   2   0   2   0   1   0   1   0   0   0   0   0\n",
    "    1   0]\n",
    " [  0   1   1   7   6   0 262  11   4   2   1   3   5   1   5   0   0   1\n",
    "    0   1]\n",
    " [  0   3   0   0   0   1   9 292   5   0   2   0   2   1   2   1   4   0\n",
    "    0   2]\n",
    " [  1   1   0   0   2   0   8   2 285   0   0   0   1   1   2   0   1   0\n",
    "    2   0]\n",
    " [  2   3   0   0   0   0   0   1   1 274   5   0   0   1   0   1   0   0\n",
    "    2   0]\n",
    " [  0   1   1   0   0   0   1   1   3  14 257   0   1   0   0   0   1   0\n",
    "    2   1]\n",
    " [  0   4   1   3   2   2   0   1   0   1   1 256   1   2   1   0   2   0\n",
    "    5   1]\n",
    " [  0  14   5   8   8   1   3   9   2   0   0   1 262   0   0   0   0   0\n",
    "    0   0]\n",
    " [  2   6   1   3   0   0   1   3   3   0   0   0   4 269   4   0   0   1\n",
    "    3   0]\n",
    " [  3   7   1   0   1   0   2   1   2   0   0   1   2   4 256   0   1   1\n",
    "    8   6]\n",
    " [  3   0   0   1   1   0   1   0   0   0   0   0   0   1   1 301   0   0\n",
    "    2   0]\n",
    " [  0   3   0   1   1   0   2   1   2   0   0   2   0   1   1   1 257   3\n",
    "   14   3]\n",
    " [  7   1   1   1   0   0   8   1   2   1   0   0   0   1   1   2   5 287\n",
    "   20   5]\n",
    " [  8   1   2   0   0   0   2   0   1   6   0   2   0   4   7   5  33  19\n",
    "  169  32]\n",
    " [ 79   2   0   1   1   0   3   0   3   1   2   0   0   2   2  17  29   1\n",
    "   22 133]]\n",
    "Classification Report :                            precision    recall  f1-score   support\n",
    "\n",
    "             alt.atheism       0.65      0.73      0.69       274\n",
    "           comp.graphics       0.76      0.80      0.78       301\n",
    " comp.os.ms-windows.misc       0.81      0.84      0.82       308\n",
    "comp.sys.ibm.pc.hardware       0.80      0.87      0.83       284\n",
    "   comp.sys.mac.hardware       0.84      0.88      0.86       297\n",
    "          comp.windows.x       0.93      0.87      0.90       295\n",
    "            misc.forsale       0.80      0.84      0.82       311\n",
    "               rec.autos       0.89      0.90      0.90       324\n",
    "         rec.motorcycles       0.89      0.93      0.91       306\n",
    "      rec.sport.baseball       0.91      0.94      0.93       290\n",
    "        rec.sport.hockey       0.95      0.91      0.93       283\n",
    "               sci.crypt       0.96      0.90      0.93       283\n",
    "         sci.electronics       0.90      0.84      0.87       313\n",
    "                 sci.med       0.92      0.90      0.91       300\n",
    "               sci.space       0.89      0.86      0.88       296\n",
    "  soc.religion.christian       0.90      0.97      0.93       311\n",
    "      talk.politics.guns       0.77      0.88      0.82       292\n",
    "   talk.politics.mideast       0.91      0.84      0.87       343\n",
    "      talk.politics.misc       0.65      0.58      0.61       291\n",
    "      talk.religion.misc       0.58      0.45      0.50       298\n",
    "\n",
    "                accuracy                           0.84      6000\n",
    "               macro avg       0.84      0.84      0.83      6000\n",
    "            weighted avg       0.84      0.84      0.84      6000\n",
    "\n",
    "\n",
    "### Code from scratch\n",
    "Accuracy = 84%\n",
    "\n",
    "Confusion Matrix :  [[200   1   0   0   0   0   0   1   4   0   1   1   0   1   1   7   2   2\n",
    "    8  45]\n",
    " [  0 241  24   4   8   7   5   0   2   0   0   1   1   2   4   0   0   0\n",
    "    1   1]\n",
    " [  0  13 255  14   2  13   3   0   0   1   0   2   2   0   1   0   0   0\n",
    "    2   0]\n",
    " [  0   3   7 246  15   0   7   2   0   0   0   0   4   0   0   0   0   0\n",
    "    0   0]\n",
    " [  0   0   7  16 259   0   9   0   0   0   0   0   5   0   0   0   0   0\n",
    "    0   1]\n",
    " [  0  15   8   3   3 259   1   0   2   0   1   0   2   0   0   0   0   0\n",
    "    1   0]\n",
    " [  0   1   1   7   7   0 261  11   4   2   1   3   5   1   5   0   0   1\n",
    "    0   1]\n",
    " [  0   3   0   0   0   1   8 292   5   0   3   0   2   1   2   1   4   0\n",
    "    0   2]\n",
    " [  1   1   0   0   2   0   7   2 285   1   0   0   1   1   1   0   1   0\n",
    "    3   0]\n",
    " [  2   3   0   0   0   0   0   1   1 274   5   0   0   1   0   1   0   0\n",
    "    2   0]\n",
    " [  0   1   1   0   0   0   1   1   2  13 259   0   1   0   0   0   1   0\n",
    "    2   1]\n",
    " [  0   4   1   3   2   2   0   1   0   1   1 257   1   2   0   0   2   0\n",
    "    6   0]\n",
    " [  0  14   5   8   8   1   3   9   2   0   0   1 262   0   0   0   0   0\n",
    "    0   0]\n",
    " [  2   6   1   3   0   0   1   3   3   0   0   0   2 271   4   0   0   1\n",
    "    3   0]\n",
    " [  3   7   1   0   1   0   2   1   2   0   0   2   1   4 256   0   1   1\n",
    "    8   6]\n",
    " [  3   0   0   0   1   0   1   0   0   0   0   0   0   1   1 302   0   0\n",
    "    2   0]\n",
    " [  0   4   0   0   1   0   2   1   1   0   0   2   0   1   1   1 256   3\n",
    "   16   3]\n",
    " [  7   1   2   1   0   0   4   1   2   1   0   0   0   1   1   2   4 290\n",
    "   20   6]\n",
    " [  8   1   2   0   0   0   2   0   1   5   0   2   0   4   6   5  32  20\n",
    "  173  30]\n",
    " [ 78   2   0   1   1   0   3   0   3   1   2   0   0   2   2  19  29   1\n",
    "   24 130]]\n",
    "Classification Report :                            precision    recall  f1-score   support\n",
    "\n",
    "             alt.atheism       0.66      0.73      0.69       274\n",
    "           comp.graphics       0.75      0.80      0.77       301\n",
    " comp.os.ms-windows.misc       0.81      0.83      0.82       308\n",
    "comp.sys.ibm.pc.hardware       0.80      0.87      0.83       284\n",
    "   comp.sys.mac.hardware       0.84      0.87      0.85       297\n",
    "          comp.windows.x       0.92      0.88      0.90       295\n",
    "            misc.forsale       0.82      0.84      0.83       311\n",
    "               rec.autos       0.90      0.90      0.90       324\n",
    "         rec.motorcycles       0.89      0.93      0.91       306\n",
    "      rec.sport.baseball       0.92      0.94      0.93       290\n",
    "        rec.sport.hockey       0.95      0.92      0.93       283\n",
    "               sci.crypt       0.95      0.91      0.93       283\n",
    "         sci.electronics       0.91      0.84      0.87       313\n",
    "                 sci.med       0.92      0.90      0.91       300\n",
    "               sci.space       0.90      0.86      0.88       296\n",
    "  soc.religion.christian       0.89      0.97      0.93       311\n",
    "      talk.politics.guns       0.77      0.88      0.82       292\n",
    "   talk.politics.mideast       0.91      0.85      0.88       343\n",
    "      talk.politics.misc       0.64      0.59      0.62       291\n",
    "      talk.religion.misc       0.58      0.44      0.50       298\n",
    "\n",
    "                accuracy                           0.84      6000\n",
    "               macro avg       0.84      0.84      0.84      6000\n",
    "            weighted avg       0.84      0.84      0.84      6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
